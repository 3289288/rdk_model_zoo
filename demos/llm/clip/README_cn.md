[English](./README.md) | 简体中文

Clip
=======

# 1. 模型介绍

CLIP（Contrastive Language-Image Pre-training）是由OpenAI开发的一种多模态（文本和图像）预训练模型。CLIP模型通过学习如何对文本和图像进行对比，从而实现跨模态的理解。这种对比学习的方法使得CLIP能够在没有任何监督标签的情况下学习到文本和图像之间的语义关系。

CLIP模型的核心思想是将文本和图像嵌入到一个共同的语义空间中，使得相关的文本描述和图像内容在这个空间中的表示彼此靠近，而不相关的则远离。这种设计使得CLIP模型能够在各种任务上表现出色，如图像分类、图像检索、文本分类等。

CLIP模型的特点：

1.**多模态嵌入**：CLIP模型首先将文本和图像分别嵌入到一个共享的多维空间中。这个空间被设计成能够捕捉文本描述和图像内容之间的语义关系。

2.**对比学习**：CLIP使用对比学习的方法来训练模型。在对比学习中，模型被要求将相关的文本描述和图像内容映射到空间中的相邻位置，而不相关的则映射到远离的位置。这样，模型学习到了如何区分相关和不相关的文本-图像对。

3.**训练数据**：CLIP使用大规模的文本和图像数据集进行预训练，其中文本描述和图像内容是从互联网上收集而来的。这些数据集包含了各种不同的文本描述和图像内容，帮助模型学习到更广泛的语义关系。

4.**自监督学习**：CLIP模型采用了自监督学习的方法，即模型在训练过程中不需要人工标注的标签。相反，模型利用数据集中的文本描述和图像内容之间的自然关联来学习。

5.**跨任务应用**：由于CLIP学习到了文本和图像之间的通用语义关系，因此可以在各种任务上进行微调，如图像分类、图像检索、文本分类等。这种通用性使得CLIP在不同领域和任务上都能取得很好的表现。

# 2. 模型下载地址

地瓜异构.bin模型文件已经上传至云服务器中，可通过 wget 命令在服务器网站中下载：

```shell
wget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_x5/text_encoder.onnx
wget https://archive.d-robotics.cc/downloads/rdk_model_zoo/rdk_x5/img_encoder.bin
```

将img_encoder.bin和text_encoder.onnx放入与当前README.md的同级目录即可。

# 3. 输入输出数据

## 2.1 Image Encoder

- 输入数据

  | 输入数据 | 数据类型 | 大小                            | 数据排布格式 |
  | -------- | -------- | ------------------------------- | ------------ |
  | image    | FLOAT32  | 1 x 3 x 224 x 224 | NCHW           |

- 输出数据

  | 输出数据 | 数据类型 | 大小                            | 数据排布格式 |
  | -------- | -------- | ------------------------------- | ------------ |
  | image_feature    | FLOAT32  | 1 x 512 | NCHW           |

## 2.2 Text Encoder

- 输入数据

  | 输入数据 | 数据类型 | 大小                            | 数据排布格式 |
  | -------- | -------- | ------------------------------- | ------------ |
  | texts    | INT32  | num_text x 77 | NCHW           |

- 输出数据

  | 输出数据 | 数据类型 | 大小                            | 数据排布格式 |
  | -------- | -------- | ------------------------------- | ------------ |
  | text_features    | FLOAT32  | feature_dim x 512 | NCHW           |